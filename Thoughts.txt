v1.00
01标准化，02归一化。正确率79%和76%存在过拟合。
试图加入第二层lstm时出现错误y_train（标签）维数与要求不合。经过试验为LSTM参数return_sequences导致。结论为：多层LSTM中前面层参数设置为true，最后一层设置为false
使用第二层lstm（标准化,relu）发现正确率仍在78%，此时训练到30次迭代出现严重过拟合现象。分析可能存在层数越多过拟合越严重的情况。
同时确认了神经网络为深度学习网络。理论上网络层数越多越好。下次试验可以采用10层及以上。考虑到层数越多，计算越久，应适当减少每层的隐藏神经元数量，减少参数。
下次试验同样要考虑正则化的方法，减少误差。
relu是否起到反作用？存疑