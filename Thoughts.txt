v1.00
01标准化，02归一化。正确率79%和76%存在过拟合。
试图加入第二层lstm时出现错误y_train（标签）维数与要求不合。经过试验为LSTM参数return_sequences导致。结论为：多层LSTM中前面层参数设置为true，最后一层设置为false
使用第二层lstm（标准化,relu）发现正确率仍在78%，此时训练到30次迭代出现严重过拟合现象。分析可能存在层数越多过拟合越严重的情况。
同时确认了神经网络为深度学习网络。理论上网络层数越多越好。下次试验可以采用10层及以上。考虑到层数越多，计算越久，应适当减少每层的隐藏神经元数量，减少参数。
下次试验同样要考虑正则化的方法，减少误差。
relu是否起到反作用？存疑
v1.10
试验了多层lstm网络，以及不同的激活函数(relu,默认)、优化器（adam,rms,adedelta），发现正确率只会下降不会上升。3456
反思之后发现，三层网络时测试集的正确率很高，但是存在严重的过拟合。尝试使用正则化手段dropout，发现正确率提高了1%
思路改变为采用大量正则化手段。“数据决定上限”
需要继续使用L1L2正则化、PCA等手段。